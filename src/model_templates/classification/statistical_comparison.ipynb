{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "predictions = {}\n",
    "acc_scores = {}\n",
    "recall_scores = {}\n",
    "f1_scores = {}\n",
    "precision_scores = {}\n",
    "cpu_times = {}\n",
    "models = []\n",
    "\n",
    "for filename in os.listdir(\"statistics\"):\n",
    "   with open(os.path.join(\"statistics\", filename), 'r') as f: # open in readonly mode\n",
    "      stats = json.load(f)\n",
    "      model_name = filename.split('.')[0]\n",
    "      predictions[model_name] = stats[\"predicted\"]\n",
    "      acc_scores[model_name] = stats[\"Accuracy Score\"]\n",
    "      recall_scores[model_name] = stats[\"Recall Score\"]\n",
    "      precision_scores[model_name] = stats[\"Precision Score\"]\n",
    "      f1_scores[model_name] = stats[\"F1 Score\"]\n",
    "      cpu_times[model_name] = stats[\"cpu time\"]\n",
    "      models.append(model_name)\n",
    "\n",
    "data = {\n",
    "   \"Acc Score\": acc_scores,\n",
    "   \"Precision Score\": precision_scores,\n",
    "   \"Recall Score\": recall_scores,\n",
    "   \"F1 Score\": f1_scores,\n",
    "   \"CPU Time\": cpu_times\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=models)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Metrics Comparison"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "colors = ['#1b9e77', '#a9f971', '#fdaa48','#6890F0','#A890F0'] \n",
    "\n",
    "\n",
    "ax[0, 0].set_title('F1 Score by model')\n",
    "ax[0, 0].set_ylabel('F1 Score')\n",
    "ax[0, 0].set_xlabel('Model')\n",
    "ax[0, 0].bar(acc_scores.keys(), acc_scores.values(), color = colors)\n",
    "\n",
    "ax[0, 1].set_title('Precision by model')\n",
    "ax[0, 1].set_ylabel('Precision')\n",
    "ax[0, 1].set_xlabel('Model')\n",
    "ax[0, 1].bar(precision_scores.keys(), precision_scores.values(), color = colors)\n",
    "\n",
    "ax[1, 0].set_title('Recall by model')\n",
    "ax[1, 0].set_ylabel('Recall')\n",
    "ax[1, 0].set_xlabel('Model')\n",
    "ax[1, 0].bar(recall_scores.keys(), recall_scores.values(), color = colors)\n",
    "\n",
    "ax[1, 1].set_title('CPU Times by model')\n",
    "ax[1, 1].set_ylabel('CPU Time')\n",
    "ax[1, 1].set_xlabel('Model')\n",
    "ax[1, 1].bar(cpu_times.keys(), cpu_times.values(), color = colors)\n",
    "\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Accuracy by model')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.bar(acc_scores.keys(), acc_scores.values(), color = colors)\n",
    "\n",
    "fig2.autofmt_xdate()\n",
    "plt.show()\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.9 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "a79bc06c91a9bf13bf775bffd87c8b82a4ff8487ca9c6ddc7ac97f9c42d293c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}